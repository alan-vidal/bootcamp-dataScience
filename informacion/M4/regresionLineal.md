## ğŸ§® 1. **RegresiÃ³n Lineal**

### ğŸ”¹ DefiniciÃ³n:
La regresiÃ³n lineal es una tÃ©cnica estadÃ­stica que modela la relaciÃ³n entre una variable dependiente (Y) y una o mÃ¡s variables independientes (X), asumiendo una relaciÃ³n lineal entre ellas.

### âœ… Utilidad:
- **Predecir valores futuros**: Por ejemplo, predecir el precio de un inmueble basado en su tamaÃ±o.
- **Explicar fenÃ³menos**: Entender cÃ³mo influye la publicidad en las ventas.
- **Modelo base**: Sirve como punto de partida antes de aplicar modelos mÃ¡s complejos (como Ã¡rboles de decisiÃ³n o redes neuronales).
- **Interpretable**: Permite entender claramente cÃ³mo afectan las variables al resultado.

### ğŸ“Š Ejemplo numÃ©rico:

| AÃ±os de Experiencia (X) | Salario Anual (en miles) (Y) |
|--------------------------|-------------------------------|
| 2                        | 50                            |
| 4                        | 60                            |
| 6                        | 70                            |
| 8                        | 80                            |
| 10                       | 90                            |

EcuaciÃ³n del modelo ajustado:
$$
\text{Salario} = 40 + 5 \cdot (\text{AÃ±os de experiencia})
$$

- **Intercepto (Î²â‚€)**: Salario inicial sin experiencia â†’ 40
- **Pendiente (Î²â‚)**: Cada aÃ±o adicional aporta $5,000 â†’ 5

---

## ğŸ“ˆ 2. **MÃ©todo de MÃ­nimos Cuadrados**

### ğŸ”¹ DefiniciÃ³n:
Es un mÃ©todo matemÃ¡tico utilizado para encontrar los coeficientes de una recta que mejor se ajuste a los datos, minimizando la suma de los cuadrados de los errores residuales.

### âœ… Utilidad:
- Garantiza la **mejor lÃ­nea de ajuste posible** dada la data.
- Es la base de muchos modelos paramÃ©tricos.
- Tiene soluciÃ³n analÃ­tica clara, lo cual es eficiente computacionalmente.

### ğŸ“Š FÃ³rmula:
$$
\min_{\beta_0, \beta_1} \sum_{i=1}^{n}(y_i - (\beta_0 + \beta_1 x_i))^2
$$

### ğŸ“Œ Ejemplo grÃ¡fico:
Imagina que graficas puntos reales y la lÃ­nea predicha. El mÃ©todo busca minimizar las distancias verticales al cuadrado entre los puntos reales y la lÃ­nea.

---

## ğŸ“ 3. **CÃ¡lculo de la Pendiente (Î²â‚)**

### ğŸ”¹ DefiniciÃ³n:
La pendiente mide cuÃ¡nto cambia Y por unidad de cambio en X. Representa la inclinaciÃ³n de la recta de regresiÃ³n.

### âœ… Utilidad:
- Indica **la direcciÃ³n y magnitud** de la relaciÃ³n entre variables.
- Es clave para interpretar el impacto de las variables independientes.

### ğŸ“Š FÃ³rmula:
$$
\beta_1 = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sum{(x_i - \bar{x})^2}} = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}
$$

### ğŸ“Œ Ejemplo:
Con los datos anteriores:

- $$ \bar{x} = 6 $$ $$ \bar{y} = 70 $$
- Calculamos covarianza y varianza manualmente:

  $$
  \text{Cov}(X,Y) = \frac{(2-6)(50-70) + (4-6)(60-70) + ...}{n-1}
  $$

  $$
  \text{Var}(X) = \frac{(2-6)^2 + (4-6)^2 + ...}{n-1}
  $$

  Finalmente:
  $$
  \beta_1 = \frac{\text{Cov}(X,Y)}{\text{Var}(X)} = 5
  $$

---

## âš–ï¸ 4. **CÃ¡lculo del Intercepto (Î²â‚€)**

### ğŸ”¹ DefiniciÃ³n:
El intercepto representa el valor de Y cuando X = 0. Es el punto donde la recta cruza el eje Y.

### âœ… Utilidad:
- Es necesario para hacer predicciones incluso cuando no hay observaciones exactas en X=0.
- Completa el modelo de regresiÃ³n junto con la pendiente.

### ğŸ“Š FÃ³rmula:
$$
\beta_0 = \bar{y} - \beta_1 \cdot \bar{x}
$$

### ğŸ“Œ Ejemplo:
Con $$ \bar{y} = 70 $$, $$ \bar{x} = 6 $$, $$ \beta_1 = 5 $$:

$$
\beta_0 = 70 - 5 \cdot 6 = 40
$$

---

## ğŸ 5. **Ejemplo en Python con scikit-learn**

### ğŸ”¹ DefiniciÃ³n:
Scikit-learn es una biblioteca de aprendizaje automÃ¡tico que permite implementar regresiÃ³n lineal fÃ¡cilmente.

### âœ… Utilidad:
- RÃ¡pida implementaciÃ³n de modelos predictivos.
- FÃ¡cil integraciÃ³n con otras herramientas de ML.
- Herramientas para mÃ©tricas, validaciÃ³n cruzada, etc.

### ğŸ§  CÃ³digo paso a paso:

```PYTHON
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Datos
X = [[1], [2], [3], [4], [5]]
y = [2, 4, 5, 4, 5]

# Modelo
model = LinearRegression()
model.fit(X, y)

# Coeficientes
print("Pendiente:", model.coef_[0])
print("Intercepto:", model.intercept_)

# Predicciones
y_pred = model.predict(X)
print("Predicciones:", y_pred)

# MÃ©tricas
print("MSE:", mean_squared_error(y, y_pred))
print("RÂ²:", r2_score(y, y_pred))

```

### ğŸ“Œ AplicaciÃ³n real:
Este tipo de cÃ³digo se usa en sistemas de recomendaciÃ³n, precios dinÃ¡micos, anÃ¡lisis de tendencias, etc.

---

## ğŸ“Š 6. **MÃ©tricas de error y evaluaciÃ³n**

### ğŸ”¹ DefiniciÃ³n:
Son indicadores que miden quÃ© tan bien funciona el modelo en comparaciÃ³n con los valores reales.

### âœ… Utilidad:
- Comparar modelos entre sÃ­.
- Validar si estÃ¡n sobreajustando o subajustando.
- Comunicar resultados a equipos tÃ©cnicos o no tÃ©cnicos.

| MÃ©trica                                            | FÃ³rmula                                                             | DescripciÃ³n                      |
| -------------------------------------------------- | ------------------------------------------------------------------- | -------------------------------- |
| **Error Cuadratico Medio (MSE)**                   | $$ \frac{1}{n} \sum{(y_i - \hat{y}_i)^2} $$                         | Promedio de errores al cuadrado  |
| **Errr absoluto medio (MAE)**                      | $$ \sqrt{\text{MSE}} $$                                             | RaÃ­z cuadrada del MSE            |
| **Coeficiente de determinacion - RÂ² (R-cuadrado)** | $$ 1 - \frac{\sum{(y_i - \hat{y}_i)^2}}{\sum{(y_i - \bar{y})^2}} $$ | ProporciÃ³n de varianza explicada |

### ğŸ“Œ Ejemplo:
Si RÂ² = 0.95, significa que el modelo explica el 95% de la variaciÃ³n en Y.

---

## ğŸ 7. **Ejemplo con Statsmodels**

### ğŸ”¹ DefiniciÃ³n:
Statsmodels es una biblioteca enfocada en anÃ¡lisis estadÃ­stico, ideal para modelos inferenciales.

### âœ… Utilidad:
- Proporciona p-valores, intervalos de confianza, estadÃ­sticas de bondad de ajuste.
- Ãštil para investigaciÃ³n y anÃ¡lisis causal.

### ğŸ§  CÃ³digo:

```PYTHON
import statsmodels.api as sm

# Datos
X = sm.add_constant([[1], [2], [3], [4], [5]])
y = [2, 4, 5, 4, 5]

# Modelo
model = sm.OLS(y, X).fit()
print(model.summary())

```

En el output:
- Puedes ver si una variable es significativa (p-valor < 0.05).
- Verificar si el modelo es Ãºtil usando el F-statistic.
- Revisar si hay colinealidad o heterocedasticidad.

---

## ğŸ“ˆ 8. **RegresiÃ³n Lineal MÃºltiple**

### ğŸ”¹ DefiniciÃ³n:
ExtensiÃ³n de la regresiÃ³n lineal simple, donde se usan mÃºltiples variables predictoras.

### âœ… Utilidad:
- Modelar relaciones mÃ¡s complejas.
- Predecir resultados considerando varias causas.
- Mejor precisiÃ³n que con solo una variable.

### ğŸ“Š EcuaciÃ³n generalizada:
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n + \epsilon
$$

### ğŸ§  CÃ³digo:

```PYTHON
from sklearn.linear_model import LinearRegression

X_multi = [[1, 2], [2, 3], [3, 4], [4, 5]]
y_multi = [2, 4, 5, 4]

model = LinearRegression().fit(X_multi, y_multi)
print("Coeficientes:", model.coef_)  # [0.5, 0.5]
print("Intercepto:", model.intercept_)

```

Modelo final:
$$
Y = 0.5 X_1 + 0.5 X_2 + 0.0
$$

---

## âš™ï¸ 9. **MÃ©todos de SelecciÃ³n del Modelo**

### ğŸ”¹ DefiniciÃ³n:
TÃ©cnicas para seleccionar las variables mÃ¡s relevantes en una regresiÃ³n lineal mÃºltiple.

### âœ… Utilidad:
- Simplificar modelos.
- Evitar overfitting.
- Mejorar interpretabilidad.
- Reducir costos computacionales.

### ğŸ“Œ Tipos:

#### â¤ Forward Selection
- Empieza sin variables.
- Agrega la que da mejor mejora (por AIC/BIC o p-valor).

#### â¤ Backward Elimination
- Empieza con todas las variables.
- Elimina la menos significativa (alta p-valor).

#### â¤ Stepwise Regression
- Mezcla de ambos: agrega y elimina variables iterativamente.

### ğŸ§  CÃ³digo de Backward Elimination:

```PYTHON
def backward_elimination(X, y, sl=0.05):
    X = np.append(arr=np.ones((len(X), 1)), values=X, axis=1)
    for i in range(X.shape[1]):
        model = sm.OLS(y, X).fit()
        max_pval = max(model.pvalues)
        if max_pval > sl:
            j = np.argmax(model.pvalues)
            X = np.delete(X, j, axis=1)
        else:
            break
    return model, X

```

---

## ğŸ¯ Resumen Final

| Concepto                      | Utilidad principal                                                                 |
|------------------------------|-------------------------------------------------------------------------------------|
| RegresiÃ³n Lineal             | Modelar relaciÃ³n entre variables y hacer predicciones                               |
| MÃ­nimos Cuadrados            | Encontrar los mejores coeficientes                                                  |
| Pendiente e Intercepto       | Interpretar cÃ³mo afectan las variables                                              |
| Scikit-learn                 | Implementar rÃ¡pido y fÃ¡cil                                                         |
| MÃ©tricas                     | Evaluar desempeÃ±o del modelo                                                      |
| Statsmodels                  | Obtener anÃ¡lisis estadÃ­stico completo                                               |
| RegresiÃ³n MÃºltiple           | Trabajar con mÃºltiples variables explicativas                                       |
| MÃ©todos de selecciÃ³n         | Seleccionar variables relevantes para mejorar el modelo                             |
